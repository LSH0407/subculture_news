#!/usr/bin/env python3
"""
X(íŠ¸ìœ„í„°) Seleniumì„ í†µí•œ ê²Œì„ ì—…ë°ì´íŠ¸ ìŠ¤í¬ë˜í•‘
í•œêµ­ ê³µì‹ ê³„ì •ì˜ ìµœì‹  íŠ¸ìœ— ìˆ˜ì§‘
"""
import sys
import io
import json
import re
import time
from datetime import datetime
from typing import List, Dict, Tuple
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Windows ì½˜ì†” ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ê³µì‹ ê³„ì •
ACCOUNTS = {
    "star_rail": "honkaisr_kr",  # ë¶•ê´´: ìŠ¤íƒ€ë ˆì¼ í•œêµ­ ê³µì‹
    "zzz": "ZZZ_KO",  # ì  ë ˆìŠ¤ ì¡´ ì œë¡œ í•œêµ­ ê³µì‹
}

# í‚¤ì›Œë“œ ê°ì§€
KEYWORDS = {
    "star_rail": ["ì›Œí”„", "ì´ë²¤íŠ¸ ì›Œí”„", "í”½ì—…", "í™•ë¥  UP", "ì¶œì‹œ", "í‚¤ë ˆë„¤", "ë£¬ë©”ì´"],
    "zzz": ["ì±„ë„", "ê¸°ê°„ í•œì •", "í”½ì—…", "í™•ë¥  UP", "ì¶œì‹œ", "ë‹¤ì´ì•„ë¦°", "Lighter"],
}

def get_selenium_driver():
    """Selenium ë“œë¼ì´ë²„ ìƒì„±"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def fetch_tweets(account: str, driver) -> List[Dict]:
    """ê³„ì •ì—ì„œ ìµœì‹  íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://x.com/{account}"
    print(f"\nFetching: {url}")
    
    try:
        driver.get(url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ íŠ¸ìœ— ë¡œë“œ
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # íŠ¸ìœ— ìš”ì†Œ ì°¾ê¸°
        # X(íŠ¸ìœ„í„°)ì˜ êµ¬ì¡°ê°€ ìì£¼ ë°”ë€Œë¯€ë¡œ ì—¬ëŸ¬ ì„ íƒì ì‹œë„
        tweet_selectors = [
            "article[data-testid='tweet']",
            "div[data-testid='tweet']",
            "article",
        ]
        
        tweets = []
        for selector in tweet_selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"  âœ… Found {len(elements)} tweets using selector: {selector}")
                
                for elem in elements[:20]:  # ìµœê·¼ 20ê°œë§Œ
                    try:
                        text = elem.text
                        if text:
                            # ë§í¬ ì¶”ì¶œ
                            links = elem.find_elements(By.TAG_NAME, "a")
                            tweet_url = ""
                            for link in links:
                                href = link.get_attribute("href")
                                if href and "/status/" in href:
                                    tweet_url = href
                                    break
                            
                            tweets.append({
                                "text": text,
                                "url": tweet_url,
                            })
                    except Exception as e:
                        continue
                
                break
        
        if not tweets:
            print(f"  âš ï¸  No tweets found")
        
        return tweets
    
    except Exception as e:
        print(f"  âŒ Error: {e}")
        return []

def extract_date_from_tweet(text: str) -> Tuple[str, str]:
    """íŠ¸ìœ—ì—ì„œ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ"""
    # íŒ¨í„´ 1: MM/DD ~ MM/DD
    m = re.search(r"(\d{1,2})/(\d{1,2})\s*[~\-â€“â€”]\s*(\d{1,2})/(\d{1,2})", text)
    if m:
        year = datetime.now().year
        start = f"{year}-{int(m.group(1)):02d}-{int(m.group(2)):02d}"
        end = f"{year}-{int(m.group(3)):02d}-{int(m.group(4)):02d}"
        return start, end
    
    # íŒ¨í„´ 2: YYYY/MM/DD ~ YYYY/MM/DD
    m = re.search(r"(\d{4})/(\d{1,2})/(\d{1,2})\s*[~\-â€“â€”]\s*(\d{4})/(\d{1,2})/(\d{1,2})", text)
    if m:
        start = f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
        end = f"{m.group(4)}-{int(m.group(5)):02d}-{int(m.group(6)):02d}"
        return start, end
    
    # íŒ¨í„´ 3: Xì›” Xì¼ ~ Xì›” Xì¼
    m = re.search(r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*[~\-â€“â€”]\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼", text)
    if m:
        year = datetime.now().year
        start = f"{year}-{int(m.group(1)):02d}-{int(m.group(2)):02d}"
        end = f"{year}-{int(m.group(3)):02d}-{int(m.group(4)):02d}"
        return start, end
    
    return "", ""

def parse_tweets(game_id: str, tweets: List[Dict]) -> List[Dict]:
    """íŠ¸ìœ—ì—ì„œ ì—…ë°ì´íŠ¸ ì •ë³´ íŒŒì‹±"""
    updates = []
    keywords = KEYWORDS.get(game_id, [])
    
    for tweet in tweets:
        text = tweet["text"]
        url = tweet["url"]
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if not any(kw in text for kw in keywords):
            continue
        
        print(f"\nğŸ” í‚¤ì›Œë“œ ê°ì§€:")
        print(f"   {text[:100]}...")
        
        # ë‚ ì§œ ì¶”ì¶œ
        start_date, end_date = extract_date_from_tweet(text)
        
        if not start_date:
            print(f"  âš ï¸  ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨")
            continue
        
        # ì œëª© ì¶”ì¶œ (ì²« ì¤„)
        title_lines = text.split('\n')
        title = title_lines[0] if title_lines else text[:60]
        
        update = {
            "game_id": game_id,
            "version": "",
            "update_date": start_date,
            "description": title,
            "url": url or f"https://x.com/{ACCOUNTS[game_id]}",
        }
        
        if end_date:
            update["end_date"] = end_date
        
        updates.append(update)
        print(f"  âœ… ì¶”ê°€: {start_date} ~ {end_date or 'N/A'} - {title[:40]}")
    
    return updates

def main():
    print("=" * 60)
    print("X(íŠ¸ìœ„í„°) Selenium ìŠ¤í¬ë˜í¼")
    print("=" * 60)
    
    driver = get_selenium_driver()
    all_updates = []
    
    try:
        # ê° ê²Œì„ë³„ë¡œ ìŠ¤í¬ë˜í•‘
        for game_id, account in ACCOUNTS.items():
            print(f"\n### {game_id.upper()} (@{account}) ###")
            
            tweets = fetch_tweets(account, driver)
            
            if not tweets:
                print(f"  âš ï¸  íŠ¸ìœ—ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            # íŠ¸ìœ— íŒŒì‹±
            updates = parse_tweets(game_id, tweets)
            all_updates.extend(updates)
            print(f"  ğŸ“Š ì´ {len(updates)}ê°œ ì—…ë°ì´íŠ¸ ê°ì§€")
    
    finally:
        driver.quit()
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    print("\n" + "=" * 60)
    print("ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©")
    print("=" * 60)
    
    try:
        with open('data/updates.json', 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
    except FileNotFoundError:
        existing_data = []
    
    print(f"ê¸°ì¡´ ì—…ë°ì´íŠ¸ ìˆ˜: {len(existing_data)}")
    print(f"ìƒˆë¡œìš´ ì—…ë°ì´íŠ¸ ìˆ˜: {len(all_updates)}")
    
    # ì¤‘ë³µ ì œê±°
    def key(u):
        return f"{u.get('game_id')}|{u.get('update_date')}|{u.get('description','')[:40]}"
    
    existing_keys = {key(u) for u in existing_data}
    added = 0
    
    for update in all_updates:
        if key(update) not in existing_keys:
            existing_data.append(update)
            added += 1
            print(f"  âœ… ì¶”ê°€: {update['game_id']} - {update['description'][:50]}")
    
    if added > 0:
        with open('data/updates.json', 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {added}ê°œ ìƒˆ ì—…ë°ì´íŠ¸ ì¶”ê°€ ì™„ë£Œ!")
    else:
        print(f"\nâ„¹ï¸  ìƒˆë¡œìš´ ì—…ë°ì´íŠ¸ ì—†ìŒ")
    
    print(f"ìµœì¢… ì—…ë°ì´íŠ¸ ìˆ˜: {len(existing_data)}")

if __name__ == "__main__":
    main()

